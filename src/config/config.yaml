# config.yaml


llm:
  llm_type: ollama
  model: llama3.1:8b-instruct-q8_0
  max_tokens: 8192
  temperature: 0.8
  api_key: null
  base_url: null